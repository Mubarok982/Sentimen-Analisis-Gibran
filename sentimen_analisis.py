# -*- coding: utf-8 -*-
"""Sentimen_analisis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fAdUiplxzWGCNtG5zksT_lNrfuZPqoXT

# Analisis Sentimen Komentar Video YouTube

Analisis ini bertujuan untuk mengetahui distribusi sentimen (Positif, Netral, Negatif) dari komentar pada video YouTube berjudul  
“Generasi Muda, Bonus Demografi dan Masa Depan Indonesia” oleh Gibran Rakabuming.

Langkah-langkahnya meliputi:
- Import library yang dibutuhkan
- Membaca data komentar dari file Excel
- melakukan cleaning data
- Melakukan preprocessing data
- membuat visualisasi wordcloud
- menerjemahkan komentar ke bahasa inggris menggunakan library `googletrans`.
- labeling komentar dengan library `transformers`.
- Menghitung jumlah komentar berdasarkan kategori sentimen
- Membuat visualisasi grafik batang untuk memudahkan interpretasi hasil
- melakukan klasifikasi komentar sentimen dengan  `naive bayes`.

# 1.Import Library
"""

import pandas as pd
import time
import seaborn as sns
import matplotlib.pyplot as plt
import os
import re

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from deep_translator import GoogleTranslator
from transformers import pipeline
os.environ["TRANSFORMERS_NO_TF"] = "1"
from googletrans import Translator
Translator = Translator()
from wordcloud import WordCloud
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""# 2.Load Dataset

"""

data = pd.read_csv('data/Youtube_Comment_Gibran.csv')
data.head()

"""# 3. Data Understanding

## 3.1 menemukan type data
"""

data.dtypes

"""## 3.2 menganalisis jumlah baris dan kolom"""

data.shape

"""## 3.3 Menganalisi data kosong"""

data.isnull().sum()

"""## 3.4 mencari data duplikat"""

data.duplicated().sum()

"""## 3.5 Melihat deskripsi Data"""

data.describe()

"""## 3.6 Menghapus komentar duplikat"""

# Hapus komentar literal "KOSONG" (tanpa peduli huruf besar/kecil)
data = data[data['Text'].str.lower() != 'kosong']

# Hapus duplikat berdasarkan isi komentar (Text)
data = data.drop_duplicates(subset='Text')

# Reset index agar rapi
data.reset_index(drop=True, inplace=True)

# Tampilkan hasil akhir
print("Jumlah data akhir:", len(data))
print(data['Text'].value_counts().head())

"""## 3.6 Deskripsi data setelah komentar duplikat di Hapus"""

data.describe()

"""# 4. Preprocessing Text"""

data.head()

"""## 4.1 Hapus kolom username"""

data = data.drop(columns='Username')
data.head()

"""## 4.2 Mengubah Text UPPERCASE ke LOWERCASE"""

data['Text'] = data['Text'].str.lower()
data.head(30)

sentiment_analysis = pipeline('sentiment-analysis')

# 1. Inisialisasi Stemmer Sastrawi
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# 2. Daftar stopwords tambahan (bahasa gaul, singkatan, dll)
stopwords_tambahan = set([
    'yg', 'yang', 'di', 'ke', 'dari', 'ga', 'gak', 'gk', 'aja', 'dong', 'deh',
    'nih', 'sih', 'lah', 'nya', 'gua', 'gue', 'loe', 'lu', 'eh', 'nya', 'si',
    'kok', 'kan', 'pun', 'pada', 'dg', 'tp', 'jd', 'krn', 'udah', 'dah', 'blm',
    'bgt', 'banget', 'abis', 'kayak', 'kayaknya', 'dll'
])

# 3. Fungsi pembersih untuk wordcloud + stemming
def clean_with_sastrawi(text):
    if pd.isna(text):
        return ''
    text = str(text).lower()
    text = re.sub(r"[^a-zA-Z\s]", " ", text)  # hanya huruf
    text = ' '.join(text.split())  # hapus spasi berlebih
    text = ' '.join([word for word in text.split() if word not in stopwords_tambahan])  # hapus stopwords
    return stemmer.stem(text)

# 4. Terapkan pembersihan pada kolom komentar asli
data['cleaned_stemmed_text'] = data['Text'].apply(clean_with_sastrawi)

# 5. Gabungkan semua komentar
all_words = ' '.join(data['cleaned_stemmed_text'])

# 6. WordCloud dari teks yang sudah distem dan dibersihkan
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud Komentar Asli (Pakai Sastrawi + Stopwords Gaul)')
plt.show()

"""# Membersihkan komentar dari simbol khusus
hal ini di lakukan karena simbol khusus bisa saja mempengaruhi hasil akhir dari labeling sentiment, di sini saya menggunakan library `re`
"""

# 1. Fungsi untuk membersihkan komentar
def clean_komen(text):
    if pd.isna(text):
        return ''
    cleaned = re.sub(r"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", str(text))
    return ' '.join(cleaned.split())

# 2. Fungsi translate aman
def safe_translate(text, target='en', retries=5, delay=5):
    if pd.isna(text) or text.strip() == '':
        return ''
    text_str = str(text)
    for i in range(retries):
        try:
            return GoogleTranslator(source='auto', target=target).translate(text_str)
        except Exception as e:
            print(f"Attempt {i+1} failed for: '{text_str[:50]}...'\n{e}")
            if i < retries - 1:
                time.sleep(delay)
            else:
                return ''

"""# Menerjemahkan komentar ke bahasa inggris
# lalu di lanjut memberikan labeling sentimen  menggunakan library`transformers`
"""

# 1. Terjemahkan komentar
print("Mulai menerjemahkan komentar...")
data['translated_text'] = data['Text'].apply(lambda x: safe_translate(x, target='en'))

# 2. Bersihkan hasil terjemahan
data['cleaned_translated_text'] = data['translated_text'].apply(clean_komen)

# 3. Analisis sentimen
classifier = pipeline("sentiment-analysis")

def classify_sentiment(text):
    if not text or text.strip() == '':
        return 'UNKNOWN'
    try:
        result = classifier(text)[0]
        label = result['label']
        score = result['score']
        return 'NEUTRAL' if score < 0.6 else label
    except Exception as e:
        print(f"Error saat analisis sentimen: {e}")
        return 'UNKNOWN'

print("Mulai analisis sentimen...")
data['sentiment'] = data['cleaned_translated_text'].apply(classify_sentiment)

# 4. Tokenisasi
data['tokens'] = data['cleaned_translated_text'].apply(lambda x: x.lower().split() if x else [])

# 5. WordCloud dari teks terjemahan yang sudah dibersihkan
all_words = ' '.join(data['cleaned_translated_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud dari Komentar Gibran (Setelah Diterjemahkan dan Dibersihkan)')
plt.show()

# 6. Tampilkan hasil akhir
print(data[['Text', 'translated_text', 'cleaned_translated_text', 'sentiment']].head())

# 1. Inisialisasi pipeline analisis sentimen dari HuggingFace
classifier = pipeline("sentiment-analysis")

# 2. Fungsi klasifikasi sentimen dengan 3 label: POSITIVE, NEGATIVE, NEUTRAL
def classify_sentiment_3label(text):
    if not text or text.strip() == '':
        return 'UNKNOWN'
    try:
        result = classifier(text)[0]
        label = result['label']  # 'POSITIVE' atau 'NEGATIVE'
        score = result['score']  # confidence score

        if score < 0.6:
            return 'NEUTRAL'
        elif label.upper() == 'POSITIVE':
            return 'POSITIVE'
        elif label.upper() == 'NEGATIVE':
            return 'NEGATIVE'
        else:
            return 'NEUTRAL'
    except Exception as e:
        print(f"Error saat klasifikasi: {e}")
        return 'UNKNOWN'

# 3. Terapkan klasifikasi sentimen ke kolom hasil terjemahan yang sudah dibersihkan
print("Melabeli komentar dengan sentimen POSITIVE, NEGATIVE, NEUTRAL...")
data['sentiment'] = data['cleaned_translated_text'].apply(classify_sentiment_3label)

# 4. Tampilkan 5 data pertama
print(data[['cleaned_translated_text', 'sentiment']].head())

plt.figure(figsize=(8,5))

# Dapatkan urutan label yang dipakai countplot
order = data['sentiment'].value_counts().index

# Plot countplot dengan urutan label yang konsisten
sns.countplot(x='sentiment', data=data, palette='Set2', order=order)

plt.title('Distribusi Sentimen Komentar Gibran')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah Komentar')

# Tampilkan jumlah di atas bar menggunakan patches
ax = plt.gca()
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width() / 2., height + 1, f'{int(height)}', ha='center')

plt.show()

# 1. Drop data yang UNKNOWN (tidak bisa diklasifikasi)
df = data[data['sentiment'] != 'UNKNOWN'].copy()

# 2. Fitur: teks komentar, Label: sentimen
X = df['cleaned_translated_text']
y = df['sentiment']

# 3. TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=3000)
X_tfidf = tfidf.fit_transform(X)

# 4. Split data train/test
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# 5. Naive Bayes Classifier
model = MultinomialNB()
model.fit(X_train, y_train)

# 6. Prediksi
y_pred = model.predict(X_test)

# 7. Evaluasi
print("Akurasi:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 8. Confusion Matrix
conf_mat = confusion_matrix(y_test, y_pred, labels=['POSITIVE', 'NEUTRAL', 'NEGATIVE'])

plt.figure(figsize=(6, 5))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['POS', 'NEU', 'NEG'], yticklabels=['POS', 'NEU', 'NEG'])
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Naive Bayes')
plt.show()

# Confusion matrix
conf_mat = confusion_matrix(y_test, y_pred, labels=['POSITIVE', 'NEUTRAL', 'NEGATIVE'])

# Visualisasi
plt.figure(figsize=(6,5))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='YlGnBu',
            xticklabels=['POSITIVE', 'NEUTRAL', 'NEGATIVE'],
            yticklabels=['POSITIVE', 'NEUTRAL', 'NEGATIVE'])
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()

# Buat DataFrame perbandingan
compare_df = pd.DataFrame({'Aktual': y_test, 'Prediksi': y_pred})

# Hitung jumlah per kombinasi (Aktual vs Prediksi)
cross_tab = pd.crosstab(compare_df['Aktual'], compare_df['Prediksi'])

# Plot sebagai bar stack
cross_tab.plot(kind='bar', stacked=True, colormap='Set2', figsize=(8,5))
plt.title('Perbandingan Sentimen Aktual vs Prediksi')
plt.xlabel('Label Aktual')
plt.ylabel('Jumlah')
plt.legend(title='Prediksi')
plt.show()

akurasi = accuracy_score(y_test, y_pred)

# Bar chart akurasi
plt.figure(figsize=(4,5))
plt.bar(['Naive Bayes'], [akurasi], color='skyblue')
plt.ylim(0, 1)
plt.title('Akurasi Model Naive Bayes')
plt.ylabel('Akurasi')
plt.text(0, akurasi + 0.02, f"{akurasi:.2%}", ha='center')
plt.show()

# Hitung jumlah setiap kategori sentimen (include UNKNOWN)
sentiment_counts = data['sentiment'].value_counts()

# Pastikan urutan yang ingin ditampilkan
order = ['POSITIVE', 'NEGATIVE', 'NEUTRAL', 'UNKNOWN']

# Buat dataframe khusus dengan urutan label yang konsisten
import pandas as pd
sentiment_df = pd.DataFrame({
    'sentiment': order,
    'count': [sentiment_counts.get(label, 0) for label in order]
})

# Plot barchart
plt.figure(figsize=(8,5))
sns.barplot(x='sentiment', y='count', data=sentiment_df, palette='Set2')

# Tambahkan jumlah di atas bar
ax = plt.gca()
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2, height + 1, f'{int(height)}', ha='center')

plt.title('Distribusi Sentimen Komentar')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah Komentar')
plt.show()

# Buat dataframe test asli dan prediksi
df_compare = pd.DataFrame({'actual': y_test, 'predicted': y_pred})

# Distribusi label asli di test set
print("Distribusi label asli:")
print(df_compare['actual'].value_counts())

# Distribusi prediksi model di test set
print("\nDistribusi prediksi model:")
print(df_compare['predicted'].value_counts())